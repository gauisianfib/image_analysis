{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nd2\n",
    "import nd2.readers\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import shutil\n",
    "import random\n",
    "import numpy as np\n",
    "import PySimpleGUI as sg\n",
    "from PIL import Image, ImageTk\n",
    "import tkinter as tk\n",
    "from tkinter import Canvas\n",
    "import yaml\n",
    "from ultralytics import YOLO\n",
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_template_files(file_path):\n",
    "        files = os.listdir(f\"{file_path}\")\n",
    "        files = [f for f in files if f.endswith(\".png\")]\n",
    "        return files\n",
    "\n",
    "\n",
    "def update_canvas(canvas, image_path):\n",
    "    global img_width, img_height\n",
    "    image = Image.open(image_path)\n",
    "    img_width, img_height = image.size\n",
    "\n",
    "    # ã‚­ãƒ£ãƒ³ãƒã‚¹ã®ã‚µã‚¤ã‚ºã‚’ç”»åƒã®ã‚µã‚¤ã‚ºã«åˆã‚ã›ã‚‹\n",
    "    canvas.config(width=img_width, height=img_height)\n",
    "\n",
    "    tk_image = ImageTk.PhotoImage(image)\n",
    "    canvas.create_image(0, 0, anchor=tk.NW, image=tk_image)\n",
    "    canvas.image = tk_image  # Keep a reference to avoid garbage collection\n",
    "\n",
    "# ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹ã‚’æç”»ã™ã‚‹ãŸã‚ã®é–¢æ•°\n",
    "def draw_bbox(canvas, x1, y1, x2, y2):\n",
    "    canvas.delete(\"bbox\")  # Clear previous bbox\n",
    "    canvas.create_rectangle(x1, y1, x2, y2, outline=\"red\", tags=\"bbox\")\n",
    "\n",
    "# Canvasã®ãƒã‚¦ã‚¹ã‚¤ãƒ™ãƒ³ãƒˆã‚’ãƒã‚¤ãƒ³ãƒ‰\n",
    "def on_canvas_button_press(event, window):\n",
    "    global start_x, start_y\n",
    "    start_x, start_y = event.x, event.y\n",
    "\n",
    "def on_canvas_button_release(event, window):\n",
    "    end_x, end_y = event.x, event.y\n",
    "    \n",
    "    # ç”»åƒã®ã‚µã‚¤ã‚ºã‚’å–å¾—\n",
    "    img_width, img_height = image.size\n",
    "    \n",
    "    # ã‚­ãƒ£ãƒ³ãƒã‚¹ã®ã‚µã‚¤ã‚ºã‚’å–å¾—\n",
    "    canvas_width, canvas_height = canvas.winfo_width(), canvas.winfo_height()\n",
    "    \n",
    "    # ã‚­ãƒ£ãƒ³ãƒã‚¹ä¸Šã®åº§æ¨™ã‚’ç”»åƒã®å®Ÿéš›ã®ãƒ”ã‚¯ã‚»ãƒ«åº§æ¨™ã«å¤‰æ›\n",
    "    start_x_img = start_x * img_width / canvas_width\n",
    "    start_y_img = start_y * img_height / canvas_height\n",
    "    end_x_img = end_x * img_width / canvas_width\n",
    "    end_y_img = end_y * img_height / canvas_height\n",
    "    \n",
    "    # ç›¸å¯¾åº§æ¨™ã«å¤‰æ›\n",
    "    x_center = (start_x_img + end_x_img) / 2 / img_width\n",
    "    y_center = (start_y_img + end_y_img) / 2 / img_height\n",
    "    width = abs(end_x_img - start_x_img) / img_width\n",
    "    height = abs(end_y_img - start_y_img) / img_height\n",
    "    \n",
    "    # YOLOå½¢å¼ã®æ–‡å­—åˆ—ã‚’ä½œæˆ\n",
    "    yolo_format = f\"{x_center:.6f} {y_center:.6f} {width:.6f} {height:.6f}\"\n",
    "    \n",
    "    # é¸æŠã•ã‚ŒãŸã‚¯ãƒ©ã‚¹ã«åº§æ¨™ã‚’è¿½åŠ \n",
    "    if window[\"-class1_radio-\"].get():  # Class1ãŒé¸æŠã•ã‚Œã¦ã„ã‚‹å ´åˆ\n",
    "        current_values = window[\"-class1-\"].get_list_values()\n",
    "        window[\"-class1-\"].update(current_values + [yolo_format])\n",
    "    elif window[\"-class2_radio-\"].get():  # Class2ãŒé¸æŠã•ã‚Œã¦ã„ã‚‹å ´åˆ\n",
    "        current_values = window[\"-class2-\"].get_list_values()\n",
    "        window[\"-class2-\"].update(current_values + [yolo_format])\n",
    "\n",
    "    # å†åº¦ç”»åƒã‚’æ›´æ–°ã—ã¦ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹ã‚’æç”»\n",
    "    img_path = os.path.join(current_dir, values[\"-name-\"][0])\n",
    "    img = Image.open(img_path)\n",
    "    class1_coords = window[\"-class1-\"].get_list_values()\n",
    "    class2_coords = window[\"-class2-\"].get_list_values()\n",
    "    img_with_boxes = draw_bounding_boxes(img, class1_coords, class2_coords)\n",
    "    update_canvas(canvas, img_with_boxes)\n",
    "\n",
    "from PIL import Image, ImageDraw, ImageTk\n",
    "import tkinter as tk\n",
    "\n",
    "def draw_bounding_boxes(image, class1_coords, class2_coords):\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    img_width, img_height = image.size\n",
    "    \n",
    "    # ã‚¯ãƒ©ã‚¹1ã®ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹ã‚’èµ¤ã§æç”»\n",
    "    for coord in class1_coords:\n",
    "        x_center, y_center, width, height = map(float, coord.split())\n",
    "        x_min = int((x_center - width / 2) * img_width)\n",
    "        y_min = int((y_center - height / 2) * img_height)\n",
    "        x_max = int((x_center + width / 2) * img_width)\n",
    "        y_max = int((y_center + height / 2) * img_height)\n",
    "        draw.rectangle([x_min, y_min, x_max, y_max], outline=\"red\", width=2)\n",
    "    \n",
    "    # ã‚¯ãƒ©ã‚¹2ã®ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹ã‚’é’ã§æç”»\n",
    "    for coord in class2_coords:\n",
    "        x_center, y_center, width, height = map(float, coord.split())\n",
    "        x_min = int((x_center - width / 2) * img_width)\n",
    "        y_min = int((y_center - height / 2) * img_height)\n",
    "        x_max = int((x_center + width / 2) * img_width)\n",
    "        y_max = int((y_center + height / 2) * img_height)\n",
    "        draw.rectangle([x_min, y_min, x_max, y_max], outline=\"blue\", width=2)\n",
    "    \n",
    "    return image\n",
    "\n",
    "# txtãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æ—¢å­˜ã®åº§æ¨™ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€é–¢æ•°\n",
    "def load_existing_annotations(txt_filepath):\n",
    "    class1_coords = []\n",
    "    class2_coords = []\n",
    "    if os.path.exists(txt_filepath):\n",
    "        with open(txt_filepath, \"r\") as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                class_id = parts[0]\n",
    "                yolo_coord = \" \".join(parts[1:])\n",
    "                if class_id == \"0\":  # class1\n",
    "                    class1_coords.append(yolo_coord)\n",
    "                elif class_id == \"1\":  # class2\n",
    "                    class2_coords.append(yolo_coord)\n",
    "    return class1_coords, class2_coords\n",
    "\n",
    "# Canvasã«ç”»åƒã‚’æ›´æ–°ã™ã‚‹é–¢æ•°\n",
    "def update_canvas(canvas, image):\n",
    "    img_width, img_height = image.size\n",
    "    canvas.config(width=img_width, height=img_height)\n",
    "    img = ImageTk.PhotoImage(image)\n",
    "    canvas.create_image(0, 0, anchor=\"nw\", image=img)\n",
    "    canvas.image = img\n",
    "\n",
    "# txtãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æ—¢å­˜ã®åº§æ¨™ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€é–¢æ•°\n",
    "def load_existing_annotations(txt_filepath):\n",
    "    class1_coords = []\n",
    "    class2_coords = []\n",
    "    if os.path.exists(txt_filepath):\n",
    "        with open(txt_filepath, \"r\") as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                class_id = parts[0]\n",
    "                yolo_coord = \" \".join(parts[1:])\n",
    "                if class_id == \"0\":  # class1\n",
    "                    class1_coords.append(yolo_coord)\n",
    "                elif class_id == \"1\":  # class2\n",
    "                    class2_coords.append(yolo_coord)\n",
    "    return class1_coords, class2_coords\n",
    "\n",
    "def adjust_brightness(image, factor):\n",
    "    \"\"\"Adjust image brightness.\"\"\"\n",
    "    return cv2.convertScaleAbs(image, alpha=factor, beta=0)\n",
    "\n",
    "def add_salt_and_pepper_noise(image, amount=0.02):\n",
    "    \"\"\"Add salt and pepper noise to the image.\"\"\"\n",
    "    noisy_image = image.copy()\n",
    "    total_pixels = image.size\n",
    "    num_salt = int(total_pixels * amount * 0.5)\n",
    "    num_pepper = int(total_pixels * amount * 0.5)\n",
    "\n",
    "    salt_coords = [np.random.randint(0, i - 1, num_salt) for i in image.shape]\n",
    "    noisy_image[salt_coords[0], salt_coords[1]] = 255\n",
    "\n",
    "    pepper_coords = [np.random.randint(0, i - 1, num_pepper) for i in image.shape]\n",
    "    noisy_image[pepper_coords[0], pepper_coords[1]] = 0\n",
    "\n",
    "    return noisy_image\n",
    "\n",
    "def random_scale(image, labels, scale_range=(0.8, 1.2)):\n",
    "    \"\"\"Randomly scale an image and adjust labels.\"\"\"\n",
    "    h, w = image.shape[:2]\n",
    "    scale = np.random.uniform(*scale_range)\n",
    "    scaled_image = cv2.resize(image, None, fx=scale, fy=scale, interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "    new_h, new_w = scaled_image.shape[:2]\n",
    "    pad_h = max(0, h - new_h)\n",
    "    pad_w = max(0, w - new_w)\n",
    "    scaled_image = cv2.copyMakeBorder(scaled_image, 0, pad_h, 0, pad_w, cv2.BORDER_CONSTANT, value=(0, 0, 0))\n",
    "\n",
    "    new_labels = []\n",
    "    for label in labels:\n",
    "        cls, x_center, y_center, width, height = label\n",
    "        x_center_scaled = x_center * scale\n",
    "        y_center_scaled = y_center * scale\n",
    "        width_scaled = width * scale\n",
    "        height_scaled = height * scale\n",
    "        if 0 <= x_center_scaled <= 1 and 0 <= y_center_scaled <= 1:\n",
    "            new_labels.append([cls, x_center_scaled, y_center_scaled, width_scaled, height_scaled])\n",
    "\n",
    "    return scaled_image, new_labels\n",
    "\n",
    "def flip_image_and_labels(image, labels, flip_code):\n",
    "    \"\"\"Flip the image and adjust labels accordingly.\"\"\"\n",
    "    h, w = image.shape[:2]\n",
    "    flipped_image = cv2.flip(image, flip_code)\n",
    "\n",
    "    flipped_labels = []\n",
    "    for label in labels:\n",
    "        cls, x_center, y_center, width, height = label\n",
    "        if flip_code == 1:\n",
    "            x_center = 1 - x_center\n",
    "        elif flip_code == 0:\n",
    "            y_center = 1 - y_center\n",
    "        elif flip_code == -1:\n",
    "            x_center = 1 - x_center\n",
    "            y_center = 1 - y_center\n",
    "        flipped_labels.append([cls, x_center, y_center, width, height])\n",
    "\n",
    "    return flipped_image, flipped_labels\n",
    "\n",
    "def augment_image_and_labels(image_path, label_path, output_image_dir, output_label_dir, augmentations):\n",
    "    \"\"\"Perform augmentations on an image and its labels.\"\"\"\n",
    "    image = cv2.imread(image_path)\n",
    "    with open(label_path, 'r') as f:\n",
    "        labels = [[float(x) if i > 0 else int(x) for i, x in enumerate(line.strip().split())] for line in f]\n",
    "\n",
    "    for aug in augmentations:\n",
    "        if aug == 'flip':\n",
    "            for flip_code in [1, 0, -1]:\n",
    "                augmented_image, augmented_labels = flip_image_and_labels(image, labels, flip_code)\n",
    "                save_augmented_data(augmented_image, augmented_labels, image_path, label_path, output_image_dir, output_label_dir, f'flip_{flip_code}')\n",
    "        elif aug == 'brightness':\n",
    "            for factor in [0.5, 1.5]:\n",
    "                augmented_image = adjust_brightness(image, factor)\n",
    "                save_augmented_data(augmented_image, labels, image_path, label_path, output_image_dir, output_label_dir, f'brightness_{factor}')\n",
    "        elif aug == 'scale':\n",
    "            augmented_image, augmented_labels = random_scale(image, labels)\n",
    "            save_augmented_data(augmented_image, augmented_labels, image_path, label_path, output_image_dir, output_label_dir, 'scale')\n",
    "        elif aug == 'noise':\n",
    "            noisy_image = add_salt_and_pepper_noise(image, amount=0.02)\n",
    "            save_augmented_data(noisy_image, labels, image_path, label_path, output_image_dir, output_label_dir, 'noise')\n",
    "\n",
    "def save_augmented_data(image, labels, image_path, label_path, output_image_dir, output_label_dir, suffix):\n",
    "    \"\"\"Save augmented image and labels.\"\"\"\n",
    "    os.makedirs(output_image_dir, exist_ok=True)\n",
    "    os.makedirs(output_label_dir, exist_ok=True)\n",
    "\n",
    "    base_name = os.path.splitext(os.path.basename(image_path))[0]\n",
    "    image_output_path = os.path.join(output_image_dir, f'{base_name}_{suffix}.jpg')\n",
    "    label_output_path = os.path.join(output_label_dir, f'{base_name}_{suffix}.txt')\n",
    "\n",
    "    cv2.imwrite(image_output_path, image)\n",
    "\n",
    "    with open(label_output_path, 'w') as f:\n",
    "        for label in labels:\n",
    "            f.write(' '.join(map(str, label)) + '\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.38 ğŸš€ Python-3.12.6 torch-2.5.1 MPS (Apple M2)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=/Users/oikawakazutaka/Desktop/ç”»åƒè§£æ/Dataset/model/dapi_EEA1.pt, data=/Users/oikawakazutaka/Desktop/ç”»åƒè§£æ/new_yolo.yaml, epochs=10, time=None, patience=100, batch=16, imgsz=416, save=True, save_period=-1, cache=False, device=mps, workers=16, project=None, name=train3, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train3\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    751702  ultralytics.nn.modules.head.Detect           [2, [64, 128, 256]]           \n",
      "Model summary: 225 layers, 3,011,238 parameters, 3,011,222 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 355/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /Users/oikawakazutaka/Desktop/ç”»åƒè§£æ/Dataset/train/labels... 952 images, 112 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 952/952 [00:00<00:00, 3660.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /Users/oikawakazutaka/Desktop/ç”»åƒè§£æ/Dataset/train/labels.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/oikawakazutaka/Desktop/ç”»åƒè§£æ/Dataset/val/labels... 13 images, 2 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:00<00:00, 778.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /Users/oikawakazutaka/Desktop/ç”»åƒè§£æ/Dataset/val/labels.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs/detect/train3/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001667, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 416 train, 416 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/train3\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/10      2.56G      1.263      1.545       1.06         96        416: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 60/60 [01:25<00:00,  1.42s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING âš ï¸ NMS time limit 2.650s exceeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         13        145    0.00247     0.0241    0.00217   0.000466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/10       2.7G       1.24      1.236      1.043         81        416: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 60/60 [02:11<00:00,  2.19s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING âš ï¸ NMS time limit 2.650s exceeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         13        145    0.00262     0.0285    0.00281   0.000372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/10      2.84G       1.21      1.122      1.026         50        416: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 60/60 [02:49<00:00,  2.83s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING âš ï¸ NMS time limit 2.650s exceeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:04<00:00,  4.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         13        145    0.00196    0.00446   0.000994   0.000298\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/10      2.99G      1.165      1.016      1.022        120        416:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 59/60 [03:13<00:03,  3.72s/it]"
     ]
    }
   ],
   "source": [
    "import PySimpleGUI as sg\n",
    "import os\n",
    "from PIL import Image, ImageTk, ImageDraw\n",
    "\n",
    "# ã‚¯ãƒªãƒƒã‚¯ã¨ãƒªãƒªãƒ¼ã‚¹ã®åº§æ¨™ã‚’è¨˜éŒ²ã™ã‚‹å¤‰æ•°\n",
    "start_x = None\n",
    "start_y = None\n",
    "\n",
    "# å·¦å´ã®ã‚«ãƒ©ãƒ ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆ\n",
    "col_left = [\n",
    "    [sg.Text(\"Image Name\", justification=\"center\", size=(20, 1))],\n",
    "    [sg.Button(\"Train\", key=\"-TrainImage-\"), sg.Button(\"Val\", key=\"-ValImage-\")],\n",
    "    [sg.Listbox(values=[], key=\"-name-\", size=(20, 40), enable_events=True)]\n",
    "]\n",
    "\n",
    "# ä¸­å¤®ã®ã‚«ãƒ©ãƒ ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆï¼ˆç”»åƒè¡¨ç¤ºï¼‰\n",
    "col_middle = [\n",
    "    [sg.Canvas(key=\"-canvas-\",size=(2048,2048))]\n",
    "]\n",
    "\n",
    "# ã‚¯ãƒ©ã‚¹è¡¨ç¤ºç”¨ã‚«ãƒ©ãƒ ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆ\n",
    "col_class = [\n",
    "    [sg.Text(\"Bounding Boxes (YOLO Format)\", justification=\"center\", size=(20, 1))],\n",
    "    [sg.Radio(\"Class 1\", \"class_choice\", key=\"-class1_radio-\", default=True)],\n",
    "    [sg.InputText(key='-INPUT1-', justification=\"center\")],\n",
    "    [sg.Listbox(values=[], key=\"-class1-\", size=(20, 20), enable_events=True, expand_x=True)],\n",
    "    [sg.Radio(\"Class 2\", \"class_choice\", key=\"-class2_radio-\")],\n",
    "    [sg.InputText(key='-INPUT2-', justification=\"center\")],\n",
    "    [sg.Listbox(values=[], key=\"-class2-\", size=(20, 20), enable_events=True, expand_x=True)]\n",
    "]\n",
    "\n",
    "# å³å´ã®ã‚«ãƒ©ãƒ ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆï¼ˆæ“ä½œãƒœã‚¿ãƒ³ï¼‰\n",
    "col_right = [\n",
    "    [sg.Text(\"Save Data\", justification=\"center\", size=(20, 1))],\n",
    "    [sg.Button(\"Save\", key=\"-save-\")],\n",
    "    [sg.Text(\"Data Augmentation\", justification=\"center\", size=(20, 1))],\n",
    "    [sg.Button(\"Data Augmentation\", key=\"-Augmentation-\")],\n",
    "    [sg.Text(\"Make.yaml\", justification=\"center\", size=(20, 1))],\n",
    "    [sg.Button(\"yaml\", key=\"-yaml-\")],\n",
    "    [sg.Text(\"Train\", justification=\"center\", size=(20, 1))],\n",
    "    [sg.Button(\"Train\", key=\"-train-\")],\n",
    "    [sg.Text(\"Go To Predict Mode\", justification=\"center\", size=(20, 1))],\n",
    "    [sg.Button(\"Go To Predict\", key=\"gotopredict\")]\n",
    "]\n",
    "\n",
    "# ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã®ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆ\n",
    "layout = [\n",
    "    [sg.Column(col_left, element_justification='center'),\n",
    "    sg.Column(col_middle, element_justification='center',scrollable=True,vertical_scroll_only=False,size=(512,512)),\n",
    "    sg.Column(col_class, element_justification='center'),\n",
    "    sg.Column(col_right, element_justification='center', vertical_alignment='top')]\n",
    "]\n",
    "\n",
    "# ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã®ä½œæˆ\n",
    "window = sg.Window(\"Train-Mode\", layout=layout, finalize=True)\n",
    "\n",
    "canvas_elem = window[\"-canvas-\"]\n",
    "canvas = canvas_elem.Widget\n",
    "\n",
    "\n",
    "canvas.bind(\"<Button-1>\", lambda event: on_canvas_button_press(event, window))\n",
    "canvas.bind(\"<ButtonRelease-1>\", lambda event: on_canvas_button_release(event, window))\n",
    "\n",
    "png_path = \"sample_img/sample_image.png\"\n",
    "# ã‚µãƒ³ãƒ—ãƒ«ç”»åƒã‚’è¡¨ç¤º\n",
    "image = Image.open(png_path)\n",
    "update_canvas(canvas, image)\n",
    "\n",
    "# é¸æŠã•ã‚Œã¦ã„ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆTrainã¾ãŸã¯Valï¼‰ã‚’ä¿æŒ\n",
    "current_dir = \"Dataset/original_process/train/image/processed_image\"\n",
    "\n",
    "# ã‚¤ãƒ™ãƒ³ãƒˆãƒ«ãƒ¼ãƒ—\n",
    "while True:\n",
    "    event, values = window.read()\n",
    "    if event == sg.WIN_CLOSED:\n",
    "        break\n",
    "\n",
    "    elif event == \"-TrainImage-\":\n",
    "\n",
    "        current_dir = \"Dataset/original_process/train/image/processed_image\"\n",
    "        file_list = os.listdir(current_dir)\n",
    "        window[\"-name-\"].update(file_list)\n",
    "\n",
    "    elif event == \"-ValImage-\":\n",
    "\n",
    "        current_dir = \"Dataset/val/images\"\n",
    "        file_list = os.listdir(current_dir)\n",
    "        window[\"-name-\"].update(file_list)\n",
    "\n",
    "    elif event == \"-name-\":\n",
    "        img_name = values[\"-name-\"][0]\n",
    "        img_path = os.path.join(current_dir, img_name)\n",
    "        image = Image.open(img_path)\n",
    "        \n",
    "        # ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹ã®åº§æ¨™ã‚’èª­ã¿è¾¼ã‚€\n",
    "        txt_filename = os.path.splitext(img_name)[0] + \".txt\"\n",
    "        train_txt_filepath = os.path.join(\"Dataset/original_process/train/label/processed_label\", txt_filename)\n",
    "        val_txt_filepath = os.path.join(\"Dataset/val/labels\", txt_filename)\n",
    "        \n",
    "        if os.path.exists(train_txt_filepath) and \"train\"in img_path:\n",
    "            txt_filepath = train_txt_filepath\n",
    "\n",
    "        elif os.path.exists(val_txt_filepath) and \"val\" in img_path:\n",
    "            txt_filepath = val_txt_filepath\n",
    "        else:\n",
    "            class1_coords, class2_coords = [], []\n",
    "        \n",
    "        if os.path.exists(txt_filepath):\n",
    "            class1_coords, class2_coords = load_existing_annotations(txt_filepath)\n",
    "        else:\n",
    "            class1_coords, class2_coords = [], []\n",
    "        \n",
    "        # ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹ã‚’æç”»ã—ã¦ç”»åƒã‚’æ›´æ–°\n",
    "        img_with_boxes = draw_bounding_boxes(image, class1_coords, class2_coords)\n",
    "        update_canvas(canvas, img_with_boxes)\n",
    "        \n",
    "        # ãƒªã‚¹ãƒˆãƒœãƒƒã‚¯ã‚¹ã‚’æ›´æ–°\n",
    "        window[\"-class1-\"].update(class1_coords)\n",
    "        window[\"-class2-\"].update(class2_coords)\n",
    "        \n",
    "    elif event == \"-save-\":\n",
    "\n",
    "        # ç¾åœ¨é¸æŠã•ã‚Œã¦ã„ã‚‹ç”»åƒã®ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆ\n",
    "        txt_filename = os.path.splitext(values[\"-name-\"][0])[0] + \".txt\"\n",
    "        \n",
    "        # ç¾åœ¨ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ãŠã‘ã‚‹ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ•ãƒ«ãƒ‘ã‚¹ã‚’ç”Ÿæˆ\n",
    "        train_txt_filepath = os.path.join(\"Dataset/original_process/train/label/processed_label\", txt_filename)\n",
    "        val_txt_filepath = os.path.join(\"Dataset/val/labels\", txt_filename)\n",
    "        \n",
    "        # ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒ train ã¾ãŸã¯ val ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«å­˜åœ¨ã™ã‚‹ã‹ç¢ºèª\n",
    "        if os.path.exists(train_txt_filepath):\n",
    "            txt_filepath = train_txt_filepath\n",
    "        elif os.path.exists(val_txt_filepath):\n",
    "            txt_filepath = val_txt_filepath\n",
    "        else:\n",
    "            sg.popup(\"å¯¾å¿œã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“\")\n",
    "            continue\n",
    "        \n",
    "        # ãƒªã‚¹ãƒˆãƒœãƒƒã‚¯ã‚¹ã‹ã‚‰åº§æ¨™ã‚’å–å¾—\n",
    "        class1_coords = window[\"-class1-\"].get_list_values()\n",
    "        class2_coords = window[\"-class2-\"].get_list_values()\n",
    "\n",
    "            \n",
    "        # ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã«åº§æ¨™ã‚’æ›¸ãè¾¼ã‚€\n",
    "        with open(txt_filepath, \"w\") as f:\n",
    "            for coord in class1_coords:\n",
    "                f.write(f\"0 {coord}\\n\")  # Class 1 ã®åº§æ¨™ã¯ã‚¯ãƒ©ã‚¹ID 0 ã§ä¿å­˜\n",
    "            for coord in class2_coords:\n",
    "                f.write(f\"1 {coord}\\n\")  # Class 2 ã®åº§æ¨™ã¯ã‚¯ãƒ©ã‚¹ID 1 ã§ä¿å­˜\n",
    "        \n",
    "        sg.popup(\"åº§æ¨™ã‚’ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã—ã¾ã—ãŸ\")\n",
    "    \n",
    "    elif event == \"-Augmentation-\":\n",
    "        # ä½¿ç”¨ä¾‹\n",
    "        image_dir = \"Dataset/original_process/train/image/processed_image\"\n",
    "        label_dir = \"Dataset/original_process/train/label/processed_label\"\n",
    "        output_image_dir = 'Dataset/train/images'\n",
    "        output_label_dir = 'Dataset/train/labels'\n",
    "\n",
    "        augmentations = ['flip', 'brightness', 'scale', 'noise']\n",
    "        for file_name in os.listdir(image_dir):\n",
    "            if file_name.endswith('.jpg') or file_name.endswith('.png'):\n",
    "                image_path = os.path.join(image_dir, file_name)\n",
    "                label_path = os.path.join(label_dir, os.path.splitext(file_name)[0] + '.txt')\n",
    "                augment_image_and_labels(image_path, label_path, output_image_dir, output_label_dir, augmentations)\n",
    "        sg.popup(\"augmentationãŒå®Œäº†ã—ã¾ã—ãŸ\")\n",
    "\n",
    "    if event == \"-yaml-\":\n",
    "        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¥åŠ›ã«åŸºã¥ã„ã¦æ–°ã—ã„YAMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ\n",
    "        new_yaml_file_path = os.path.join(os.getcwd(), 'new_yolo.yaml')\n",
    "        print(f\"New YAML file path: {new_yaml_file_path}\")\n",
    "        print(f\"Input 1: {values['-INPUT1-']}\")\n",
    "        print(f\"Input 2: {values['-INPUT2-']}\")\n",
    "\n",
    "        try:\n",
    "            # æ–°ã—ã„YAMLãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ\n",
    "            new_data = {\n",
    "                'train': 'Dataset/train/images',\n",
    "                'val': 'Dataset/val/images',\n",
    "                'nc': 2,  # ã‚¯ãƒ©ã‚¹æ•°ã‚’2ã«è¨­å®š\n",
    "                'names': {\n",
    "                    0: values[\"-INPUT1-\"],\n",
    "                    1: values[\"-INPUT2-\"]\n",
    "                }\n",
    "            }\n",
    "\n",
    "            # æ–°ã—ã„YAMLãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãè¾¼ã‚€\n",
    "            with open(new_yaml_file_path, 'w') as file:\n",
    "                yaml.dump(new_data, file, default_flow_style=False, sort_keys=False)\n",
    "\n",
    "            sg.popup(\"æ–°ã—ã„yamlãƒ•ã‚¡ã‚¤ãƒ«ãŒæ­£å¸¸ã«ä½œæˆã•ã‚Œã¾ã—ãŸ\")\n",
    "\n",
    "        except Exception as e:\n",
    "            sg.popup(f\"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}\")\n",
    "\n",
    "    if event == \"-train-\":\n",
    "        # ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ï¼ˆä¾‹: YOLOv8nï¼‰\n",
    "        model = YOLO('yolov8n.pt')\n",
    "        answer = sg.popup_yes_no(\"å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã—ã¾ã™ã‹ï¼Ÿ\")\n",
    "        if answer == 'No':\n",
    "            model = YOLO('yolov8n.pt')\n",
    "        elif answer == 'Yes':\n",
    "            model_path = sg.popup_get_file(\".ptãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„\")\n",
    "            if model_path and model_path.endswith('.pt'):\n",
    "                sg.popup(f\"{model_path}ãŒæ­£å¸¸ã«èª­ã¿è¾¼ã¾ã‚Œã¾ã—ãŸ\")\n",
    "                model = YOLO(model_path)\n",
    "            else:\n",
    "                sg.popup(\"èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚yolov8n.ptã‚’ä½¿ã£ã¦å­¦ç¿’ã‚’é–‹å§‹ã—ã¾ã™\")\n",
    "                model = YOLO('yolov8n.pt')\n",
    "\n",
    "        # MPSãƒ‡ãƒã‚¤ã‚¹ã®ç¢ºèª\n",
    "        device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "        sg.popup(f\"ä½¿ç”¨ã™ã‚‹ãƒ‡ãƒã‚¤ã‚¹: {device}\")\n",
    "\n",
    "        # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®å®Ÿè¡Œ\n",
    "        results = model.train(\n",
    "            data=f'{os.getcwd()}/new_yolo.yaml',  # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®è¨­å®š\n",
    "            epochs=10,             # ã‚¨ãƒãƒƒã‚¯æ•°\n",
    "            batch=16,              # ãƒãƒƒãƒã‚µã‚¤ã‚º\n",
    "            imgsz=416,             # ç”»åƒã‚µã‚¤ã‚º\n",
    "            workers=16,            # ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°\n",
    "            device=device,         # ãƒ‡ãƒã‚¤ã‚¹è¨­å®šï¼ˆMPSã¾ãŸã¯CPUä½¿ç”¨ï¼‰\n",
    "            cache=False\n",
    "        )\n",
    "\n",
    "        # å­¦ç¿’å®Œäº†å¾Œã®ãƒ¢ãƒ‡ãƒ«ä¿å­˜\n",
    "        model_name = sg.popup_get_text(\"å­¦ç¿’ãŒå®Œäº†ã—ã¾ã—ãŸã€‚ãƒ¢ãƒ‡ãƒ«ã®åå‰ã‚’è¨˜å…¥ã—ã¦ãã ã•ã„\")\n",
    "        if model_name:\n",
    "            model.save(f'Dataset/model/{model_name}.pt')\n",
    "            sg.popup(f\"ãƒ¢ãƒ‡ãƒ«ãŒä¿å­˜ã•ã‚Œã¾ã—ãŸ: {model_name}.pt\")\n",
    "        else:\n",
    "            sg.popup(\"ãƒ¢ãƒ‡ãƒ«åãŒæœªå…¥åŠ›ã®ãŸã‚ã€ä¿å­˜ã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚\")\n",
    "\n",
    "\n",
    "window.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
