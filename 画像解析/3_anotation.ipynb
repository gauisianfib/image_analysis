{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nd2\n",
    "import nd2.readers\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import shutil\n",
    "import random\n",
    "import numpy as np\n",
    "import PySimpleGUI as sg\n",
    "from PIL import Image, ImageTk\n",
    "import tkinter as tk\n",
    "from tkinter import Canvas\n",
    "import yaml\n",
    "from ultralytics import YOLO\n",
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_template_files(file_path):\n",
    "        files = os.listdir(f\"{file_path}\")\n",
    "        files = [f for f in files if f.endswith(\".png\")]\n",
    "        return files\n",
    "\n",
    "\n",
    "def update_canvas(canvas, image_path):\n",
    "    global img_width, img_height\n",
    "    image = Image.open(image_path)\n",
    "    img_width, img_height = image.size\n",
    "\n",
    "    # キャンバスのサイズを画像のサイズに合わせる\n",
    "    canvas.config(width=img_width, height=img_height)\n",
    "\n",
    "    tk_image = ImageTk.PhotoImage(image)\n",
    "    canvas.create_image(0, 0, anchor=tk.NW, image=tk_image)\n",
    "    canvas.image = tk_image  # Keep a reference to avoid garbage collection\n",
    "\n",
    "# バウンディングボックスを描画するための関数\n",
    "def draw_bbox(canvas, x1, y1, x2, y2):\n",
    "    canvas.delete(\"bbox\")  # Clear previous bbox\n",
    "    canvas.create_rectangle(x1, y1, x2, y2, outline=\"red\", tags=\"bbox\")\n",
    "\n",
    "# Canvasのマウスイベントをバインド\n",
    "def on_canvas_button_press(event, window):\n",
    "    global start_x, start_y\n",
    "    start_x, start_y = event.x, event.y\n",
    "\n",
    "def on_canvas_button_release(event, window):\n",
    "    end_x, end_y = event.x, event.y\n",
    "    \n",
    "    # 画像のサイズを取得\n",
    "    img_width, img_height = image.size\n",
    "    \n",
    "    # キャンバスのサイズを取得\n",
    "    canvas_width, canvas_height = canvas.winfo_width(), canvas.winfo_height()\n",
    "    \n",
    "    # キャンバス上の座標を画像の実際のピクセル座標に変換\n",
    "    start_x_img = start_x * img_width / canvas_width\n",
    "    start_y_img = start_y * img_height / canvas_height\n",
    "    end_x_img = end_x * img_width / canvas_width\n",
    "    end_y_img = end_y * img_height / canvas_height\n",
    "    \n",
    "    # 相対座標に変換\n",
    "    x_center = (start_x_img + end_x_img) / 2 / img_width\n",
    "    y_center = (start_y_img + end_y_img) / 2 / img_height\n",
    "    width = abs(end_x_img - start_x_img) / img_width\n",
    "    height = abs(end_y_img - start_y_img) / img_height\n",
    "    \n",
    "    # YOLO形式の文字列を作成\n",
    "    yolo_format = f\"{x_center:.6f} {y_center:.6f} {width:.6f} {height:.6f}\"\n",
    "    \n",
    "    # 選択されたクラスに座標を追加\n",
    "    if window[\"-class1_radio-\"].get():  # Class1が選択されている場合\n",
    "        current_values = window[\"-class1-\"].get_list_values()\n",
    "        window[\"-class1-\"].update(current_values + [yolo_format])\n",
    "    elif window[\"-class2_radio-\"].get():  # Class2が選択されている場合\n",
    "        current_values = window[\"-class2-\"].get_list_values()\n",
    "        window[\"-class2-\"].update(current_values + [yolo_format])\n",
    "\n",
    "    # 再度画像を更新してバウンディングボックスを描画\n",
    "    img_path = os.path.join(current_dir, values[\"-name-\"][0])\n",
    "    img = Image.open(img_path)\n",
    "    class1_coords = window[\"-class1-\"].get_list_values()\n",
    "    class2_coords = window[\"-class2-\"].get_list_values()\n",
    "    img_with_boxes = draw_bounding_boxes(img, class1_coords, class2_coords)\n",
    "    update_canvas(canvas, img_with_boxes)\n",
    "\n",
    "from PIL import Image, ImageDraw, ImageTk\n",
    "import tkinter as tk\n",
    "\n",
    "def draw_bounding_boxes(image, class1_coords, class2_coords):\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    img_width, img_height = image.size\n",
    "    \n",
    "    # クラス1のバウンディングボックスを赤で描画\n",
    "    for coord in class1_coords:\n",
    "        x_center, y_center, width, height = map(float, coord.split())\n",
    "        x_min = int((x_center - width / 2) * img_width)\n",
    "        y_min = int((y_center - height / 2) * img_height)\n",
    "        x_max = int((x_center + width / 2) * img_width)\n",
    "        y_max = int((y_center + height / 2) * img_height)\n",
    "        draw.rectangle([x_min, y_min, x_max, y_max], outline=\"red\", width=2)\n",
    "    \n",
    "    # クラス2のバウンディングボックスを青で描画\n",
    "    for coord in class2_coords:\n",
    "        x_center, y_center, width, height = map(float, coord.split())\n",
    "        x_min = int((x_center - width / 2) * img_width)\n",
    "        y_min = int((y_center - height / 2) * img_height)\n",
    "        x_max = int((x_center + width / 2) * img_width)\n",
    "        y_max = int((y_center + height / 2) * img_height)\n",
    "        draw.rectangle([x_min, y_min, x_max, y_max], outline=\"blue\", width=2)\n",
    "    \n",
    "    return image\n",
    "\n",
    "# txtファイルから既存の座標データを読み込む関数\n",
    "def load_existing_annotations(txt_filepath):\n",
    "    class1_coords = []\n",
    "    class2_coords = []\n",
    "    if os.path.exists(txt_filepath):\n",
    "        with open(txt_filepath, \"r\") as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                class_id = parts[0]\n",
    "                yolo_coord = \" \".join(parts[1:])\n",
    "                if class_id == \"0\":  # class1\n",
    "                    class1_coords.append(yolo_coord)\n",
    "                elif class_id == \"1\":  # class2\n",
    "                    class2_coords.append(yolo_coord)\n",
    "    return class1_coords, class2_coords\n",
    "\n",
    "# Canvasに画像を更新する関数\n",
    "def update_canvas(canvas, image):\n",
    "    img_width, img_height = image.size\n",
    "    canvas.config(width=img_width, height=img_height)\n",
    "    img = ImageTk.PhotoImage(image)\n",
    "    canvas.create_image(0, 0, anchor=\"nw\", image=img)\n",
    "    canvas.image = img\n",
    "\n",
    "# txtファイルから既存の座標データを読み込む関数\n",
    "def load_existing_annotations(txt_filepath):\n",
    "    class1_coords = []\n",
    "    class2_coords = []\n",
    "    if os.path.exists(txt_filepath):\n",
    "        with open(txt_filepath, \"r\") as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                class_id = parts[0]\n",
    "                yolo_coord = \" \".join(parts[1:])\n",
    "                if class_id == \"0\":  # class1\n",
    "                    class1_coords.append(yolo_coord)\n",
    "                elif class_id == \"1\":  # class2\n",
    "                    class2_coords.append(yolo_coord)\n",
    "    return class1_coords, class2_coords\n",
    "\n",
    "def adjust_brightness(image, factor):\n",
    "    \"\"\"Adjust image brightness.\"\"\"\n",
    "    return cv2.convertScaleAbs(image, alpha=factor, beta=0)\n",
    "\n",
    "def add_salt_and_pepper_noise(image, amount=0.02):\n",
    "    \"\"\"Add salt and pepper noise to the image.\"\"\"\n",
    "    noisy_image = image.copy()\n",
    "    total_pixels = image.size\n",
    "    num_salt = int(total_pixels * amount * 0.5)\n",
    "    num_pepper = int(total_pixels * amount * 0.5)\n",
    "\n",
    "    salt_coords = [np.random.randint(0, i - 1, num_salt) for i in image.shape]\n",
    "    noisy_image[salt_coords[0], salt_coords[1]] = 255\n",
    "\n",
    "    pepper_coords = [np.random.randint(0, i - 1, num_pepper) for i in image.shape]\n",
    "    noisy_image[pepper_coords[0], pepper_coords[1]] = 0\n",
    "\n",
    "    return noisy_image\n",
    "\n",
    "def random_scale(image, labels, scale_range=(0.8, 1.2)):\n",
    "    \"\"\"Randomly scale an image and adjust labels.\"\"\"\n",
    "    h, w = image.shape[:2]\n",
    "    scale = np.random.uniform(*scale_range)\n",
    "    scaled_image = cv2.resize(image, None, fx=scale, fy=scale, interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "    new_h, new_w = scaled_image.shape[:2]\n",
    "    pad_h = max(0, h - new_h)\n",
    "    pad_w = max(0, w - new_w)\n",
    "    scaled_image = cv2.copyMakeBorder(scaled_image, 0, pad_h, 0, pad_w, cv2.BORDER_CONSTANT, value=(0, 0, 0))\n",
    "\n",
    "    new_labels = []\n",
    "    for label in labels:\n",
    "        cls, x_center, y_center, width, height = label\n",
    "        x_center_scaled = x_center * scale\n",
    "        y_center_scaled = y_center * scale\n",
    "        width_scaled = width * scale\n",
    "        height_scaled = height * scale\n",
    "        if 0 <= x_center_scaled <= 1 and 0 <= y_center_scaled <= 1:\n",
    "            new_labels.append([cls, x_center_scaled, y_center_scaled, width_scaled, height_scaled])\n",
    "\n",
    "    return scaled_image, new_labels\n",
    "\n",
    "def flip_image_and_labels(image, labels, flip_code):\n",
    "    \"\"\"Flip the image and adjust labels accordingly.\"\"\"\n",
    "    h, w = image.shape[:2]\n",
    "    flipped_image = cv2.flip(image, flip_code)\n",
    "\n",
    "    flipped_labels = []\n",
    "    for label in labels:\n",
    "        cls, x_center, y_center, width, height = label\n",
    "        if flip_code == 1:\n",
    "            x_center = 1 - x_center\n",
    "        elif flip_code == 0:\n",
    "            y_center = 1 - y_center\n",
    "        elif flip_code == -1:\n",
    "            x_center = 1 - x_center\n",
    "            y_center = 1 - y_center\n",
    "        flipped_labels.append([cls, x_center, y_center, width, height])\n",
    "\n",
    "    return flipped_image, flipped_labels\n",
    "\n",
    "def augment_image_and_labels(image_path, label_path, output_image_dir, output_label_dir, augmentations):\n",
    "    \"\"\"Perform augmentations on an image and its labels.\"\"\"\n",
    "    image = cv2.imread(image_path)\n",
    "    with open(label_path, 'r') as f:\n",
    "        labels = [[float(x) if i > 0 else int(x) for i, x in enumerate(line.strip().split())] for line in f]\n",
    "\n",
    "    for aug in augmentations:\n",
    "        if aug == 'flip':\n",
    "            for flip_code in [1, 0, -1]:\n",
    "                augmented_image, augmented_labels = flip_image_and_labels(image, labels, flip_code)\n",
    "                save_augmented_data(augmented_image, augmented_labels, image_path, label_path, output_image_dir, output_label_dir, f'flip_{flip_code}')\n",
    "        elif aug == 'brightness':\n",
    "            for factor in [0.5, 1.5]:\n",
    "                augmented_image = adjust_brightness(image, factor)\n",
    "                save_augmented_data(augmented_image, labels, image_path, label_path, output_image_dir, output_label_dir, f'brightness_{factor}')\n",
    "        elif aug == 'scale':\n",
    "            augmented_image, augmented_labels = random_scale(image, labels)\n",
    "            save_augmented_data(augmented_image, augmented_labels, image_path, label_path, output_image_dir, output_label_dir, 'scale')\n",
    "        elif aug == 'noise':\n",
    "            noisy_image = add_salt_and_pepper_noise(image, amount=0.02)\n",
    "            save_augmented_data(noisy_image, labels, image_path, label_path, output_image_dir, output_label_dir, 'noise')\n",
    "\n",
    "def save_augmented_data(image, labels, image_path, label_path, output_image_dir, output_label_dir, suffix):\n",
    "    \"\"\"Save augmented image and labels.\"\"\"\n",
    "    os.makedirs(output_image_dir, exist_ok=True)\n",
    "    os.makedirs(output_label_dir, exist_ok=True)\n",
    "\n",
    "    base_name = os.path.splitext(os.path.basename(image_path))[0]\n",
    "    image_output_path = os.path.join(output_image_dir, f'{base_name}_{suffix}.jpg')\n",
    "    label_output_path = os.path.join(output_label_dir, f'{base_name}_{suffix}.txt')\n",
    "\n",
    "    cv2.imwrite(image_output_path, image)\n",
    "\n",
    "    with open(label_output_path, 'w') as f:\n",
    "        for label in labels:\n",
    "            f.write(' '.join(map(str, label)) + '\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.38 🚀 Python-3.12.6 torch-2.5.1 MPS (Apple M2)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=/Users/oikawakazutaka/Desktop/画像解析/Dataset/model/dapi_EEA1.pt, data=/Users/oikawakazutaka/Desktop/画像解析/new_yolo.yaml, epochs=10, time=None, patience=100, batch=16, imgsz=416, save=True, save_period=-1, cache=False, device=mps, workers=16, project=None, name=train3, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train3\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    751702  ultralytics.nn.modules.head.Detect           [2, [64, 128, 256]]           \n",
      "Model summary: 225 layers, 3,011,238 parameters, 3,011,222 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 355/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /Users/oikawakazutaka/Desktop/画像解析/Dataset/train/labels... 952 images, 112 backgrounds, 0 corrupt: 100%|██████████| 952/952 [00:00<00:00, 3660.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /Users/oikawakazutaka/Desktop/画像解析/Dataset/train/labels.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/oikawakazutaka/Desktop/画像解析/Dataset/val/labels... 13 images, 2 backgrounds, 0 corrupt: 100%|██████████| 13/13 [00:00<00:00, 778.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /Users/oikawakazutaka/Desktop/画像解析/Dataset/val/labels.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs/detect/train3/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001667, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 416 train, 416 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/train3\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/10      2.56G      1.263      1.545       1.06         96        416: 100%|██████████| 60/60 [01:25<00:00,  1.42s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ NMS time limit 2.650s exceeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:12<00:00, 12.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         13        145    0.00247     0.0241    0.00217   0.000466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/10       2.7G       1.24      1.236      1.043         81        416: 100%|██████████| 60/60 [02:11<00:00,  2.19s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ NMS time limit 2.650s exceeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:06<00:00,  6.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         13        145    0.00262     0.0285    0.00281   0.000372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/10      2.84G       1.21      1.122      1.026         50        416: 100%|██████████| 60/60 [02:49<00:00,  2.83s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ NMS time limit 2.650s exceeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:04<00:00,  4.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         13        145    0.00196    0.00446   0.000994   0.000298\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/10      2.99G      1.165      1.016      1.022        120        416:  98%|█████████▊| 59/60 [03:13<00:03,  3.72s/it]"
     ]
    }
   ],
   "source": [
    "import PySimpleGUI as sg\n",
    "import os\n",
    "from PIL import Image, ImageTk, ImageDraw\n",
    "\n",
    "# クリックとリリースの座標を記録する変数\n",
    "start_x = None\n",
    "start_y = None\n",
    "\n",
    "# 左側のカラムレイアウト\n",
    "col_left = [\n",
    "    [sg.Text(\"Image Name\", justification=\"center\", size=(20, 1))],\n",
    "    [sg.Button(\"Train\", key=\"-TrainImage-\"), sg.Button(\"Val\", key=\"-ValImage-\")],\n",
    "    [sg.Listbox(values=[], key=\"-name-\", size=(20, 40), enable_events=True)]\n",
    "]\n",
    "\n",
    "# 中央のカラムレイアウト（画像表示）\n",
    "col_middle = [\n",
    "    [sg.Canvas(key=\"-canvas-\",size=(2048,2048))]\n",
    "]\n",
    "\n",
    "# クラス表示用カラムレイアウト\n",
    "col_class = [\n",
    "    [sg.Text(\"Bounding Boxes (YOLO Format)\", justification=\"center\", size=(20, 1))],\n",
    "    [sg.Radio(\"Class 1\", \"class_choice\", key=\"-class1_radio-\", default=True)],\n",
    "    [sg.InputText(key='-INPUT1-', justification=\"center\")],\n",
    "    [sg.Listbox(values=[], key=\"-class1-\", size=(20, 20), enable_events=True, expand_x=True)],\n",
    "    [sg.Radio(\"Class 2\", \"class_choice\", key=\"-class2_radio-\")],\n",
    "    [sg.InputText(key='-INPUT2-', justification=\"center\")],\n",
    "    [sg.Listbox(values=[], key=\"-class2-\", size=(20, 20), enable_events=True, expand_x=True)]\n",
    "]\n",
    "\n",
    "# 右側のカラムレイアウト（操作ボタン）\n",
    "col_right = [\n",
    "    [sg.Text(\"Save Data\", justification=\"center\", size=(20, 1))],\n",
    "    [sg.Button(\"Save\", key=\"-save-\")],\n",
    "    [sg.Text(\"Data Augmentation\", justification=\"center\", size=(20, 1))],\n",
    "    [sg.Button(\"Data Augmentation\", key=\"-Augmentation-\")],\n",
    "    [sg.Text(\"Make.yaml\", justification=\"center\", size=(20, 1))],\n",
    "    [sg.Button(\"yaml\", key=\"-yaml-\")],\n",
    "    [sg.Text(\"Train\", justification=\"center\", size=(20, 1))],\n",
    "    [sg.Button(\"Train\", key=\"-train-\")],\n",
    "    [sg.Text(\"Go To Predict Mode\", justification=\"center\", size=(20, 1))],\n",
    "    [sg.Button(\"Go To Predict\", key=\"gotopredict\")]\n",
    "]\n",
    "\n",
    "# ウィンドウのレイアウト\n",
    "layout = [\n",
    "    [sg.Column(col_left, element_justification='center'),\n",
    "    sg.Column(col_middle, element_justification='center',scrollable=True,vertical_scroll_only=False,size=(512,512)),\n",
    "    sg.Column(col_class, element_justification='center'),\n",
    "    sg.Column(col_right, element_justification='center', vertical_alignment='top')]\n",
    "]\n",
    "\n",
    "# ウィンドウの作成\n",
    "window = sg.Window(\"Train-Mode\", layout=layout, finalize=True)\n",
    "\n",
    "canvas_elem = window[\"-canvas-\"]\n",
    "canvas = canvas_elem.Widget\n",
    "\n",
    "\n",
    "canvas.bind(\"<Button-1>\", lambda event: on_canvas_button_press(event, window))\n",
    "canvas.bind(\"<ButtonRelease-1>\", lambda event: on_canvas_button_release(event, window))\n",
    "\n",
    "png_path = \"sample_img/sample_image.png\"\n",
    "# サンプル画像を表示\n",
    "image = Image.open(png_path)\n",
    "update_canvas(canvas, image)\n",
    "\n",
    "# 選択されているディレクトリ（TrainまたはVal）を保持\n",
    "current_dir = \"Dataset/original_process/train/image/processed_image\"\n",
    "\n",
    "# イベントループ\n",
    "while True:\n",
    "    event, values = window.read()\n",
    "    if event == sg.WIN_CLOSED:\n",
    "        break\n",
    "\n",
    "    elif event == \"-TrainImage-\":\n",
    "\n",
    "        current_dir = \"Dataset/original_process/train/image/processed_image\"\n",
    "        file_list = os.listdir(current_dir)\n",
    "        window[\"-name-\"].update(file_list)\n",
    "\n",
    "    elif event == \"-ValImage-\":\n",
    "\n",
    "        current_dir = \"Dataset/val/images\"\n",
    "        file_list = os.listdir(current_dir)\n",
    "        window[\"-name-\"].update(file_list)\n",
    "\n",
    "    elif event == \"-name-\":\n",
    "        img_name = values[\"-name-\"][0]\n",
    "        img_path = os.path.join(current_dir, img_name)\n",
    "        image = Image.open(img_path)\n",
    "        \n",
    "        # バウンディングボックスの座標を読み込む\n",
    "        txt_filename = os.path.splitext(img_name)[0] + \".txt\"\n",
    "        train_txt_filepath = os.path.join(\"Dataset/original_process/train/label/processed_label\", txt_filename)\n",
    "        val_txt_filepath = os.path.join(\"Dataset/val/labels\", txt_filename)\n",
    "        \n",
    "        if os.path.exists(train_txt_filepath) and \"train\"in img_path:\n",
    "            txt_filepath = train_txt_filepath\n",
    "\n",
    "        elif os.path.exists(val_txt_filepath) and \"val\" in img_path:\n",
    "            txt_filepath = val_txt_filepath\n",
    "        else:\n",
    "            class1_coords, class2_coords = [], []\n",
    "        \n",
    "        if os.path.exists(txt_filepath):\n",
    "            class1_coords, class2_coords = load_existing_annotations(txt_filepath)\n",
    "        else:\n",
    "            class1_coords, class2_coords = [], []\n",
    "        \n",
    "        # バウンディングボックスを描画して画像を更新\n",
    "        img_with_boxes = draw_bounding_boxes(image, class1_coords, class2_coords)\n",
    "        update_canvas(canvas, img_with_boxes)\n",
    "        \n",
    "        # リストボックスを更新\n",
    "        window[\"-class1-\"].update(class1_coords)\n",
    "        window[\"-class2-\"].update(class2_coords)\n",
    "        \n",
    "    elif event == \"-save-\":\n",
    "\n",
    "        # 現在選択されている画像のファイル名からテキストファイル名を生成\n",
    "        txt_filename = os.path.splitext(values[\"-name-\"][0])[0] + \".txt\"\n",
    "        \n",
    "        # 現在のディレクトリにおけるテキストファイルのフルパスを生成\n",
    "        train_txt_filepath = os.path.join(\"Dataset/original_process/train/label/processed_label\", txt_filename)\n",
    "        val_txt_filepath = os.path.join(\"Dataset/val/labels\", txt_filename)\n",
    "        \n",
    "        # テキストファイルが train または val ディレクトリに存在するか確認\n",
    "        if os.path.exists(train_txt_filepath):\n",
    "            txt_filepath = train_txt_filepath\n",
    "        elif os.path.exists(val_txt_filepath):\n",
    "            txt_filepath = val_txt_filepath\n",
    "        else:\n",
    "            sg.popup(\"対応するテキストファイルが見つかりません\")\n",
    "            continue\n",
    "        \n",
    "        # リストボックスから座標を取得\n",
    "        class1_coords = window[\"-class1-\"].get_list_values()\n",
    "        class2_coords = window[\"-class2-\"].get_list_values()\n",
    "\n",
    "            \n",
    "        # テキストファイルに座標を書き込む\n",
    "        with open(txt_filepath, \"w\") as f:\n",
    "            for coord in class1_coords:\n",
    "                f.write(f\"0 {coord}\\n\")  # Class 1 の座標はクラスID 0 で保存\n",
    "            for coord in class2_coords:\n",
    "                f.write(f\"1 {coord}\\n\")  # Class 2 の座標はクラスID 1 で保存\n",
    "        \n",
    "        sg.popup(\"座標をテキストファイルに保存しました\")\n",
    "    \n",
    "    elif event == \"-Augmentation-\":\n",
    "        # 使用例\n",
    "        image_dir = \"Dataset/original_process/train/image/processed_image\"\n",
    "        label_dir = \"Dataset/original_process/train/label/processed_label\"\n",
    "        output_image_dir = 'Dataset/train/images'\n",
    "        output_label_dir = 'Dataset/train/labels'\n",
    "\n",
    "        augmentations = ['flip', 'brightness', 'scale', 'noise']\n",
    "        for file_name in os.listdir(image_dir):\n",
    "            if file_name.endswith('.jpg') or file_name.endswith('.png'):\n",
    "                image_path = os.path.join(image_dir, file_name)\n",
    "                label_path = os.path.join(label_dir, os.path.splitext(file_name)[0] + '.txt')\n",
    "                augment_image_and_labels(image_path, label_path, output_image_dir, output_label_dir, augmentations)\n",
    "        sg.popup(\"augmentationが完了しました\")\n",
    "\n",
    "    if event == \"-yaml-\":\n",
    "        # ユーザーの入力に基づいて新しいYAMLファイルを作成\n",
    "        new_yaml_file_path = os.path.join(os.getcwd(), 'new_yolo.yaml')\n",
    "        print(f\"New YAML file path: {new_yaml_file_path}\")\n",
    "        print(f\"Input 1: {values['-INPUT1-']}\")\n",
    "        print(f\"Input 2: {values['-INPUT2-']}\")\n",
    "\n",
    "        try:\n",
    "            # 新しいYAMLデータを作成\n",
    "            new_data = {\n",
    "                'train': 'Dataset/train/images',\n",
    "                'val': 'Dataset/val/images',\n",
    "                'nc': 2,  # クラス数を2に設定\n",
    "                'names': {\n",
    "                    0: values[\"-INPUT1-\"],\n",
    "                    1: values[\"-INPUT2-\"]\n",
    "                }\n",
    "            }\n",
    "\n",
    "            # 新しいYAMLファイルに書き込む\n",
    "            with open(new_yaml_file_path, 'w') as file:\n",
    "                yaml.dump(new_data, file, default_flow_style=False, sort_keys=False)\n",
    "\n",
    "            sg.popup(\"新しいyamlファイルが正常に作成されました\")\n",
    "\n",
    "        except Exception as e:\n",
    "            sg.popup(f\"エラーが発生しました: {e}\")\n",
    "\n",
    "    if event == \"-train-\":\n",
    "        # モデルのロード（例: YOLOv8n）\n",
    "        model = YOLO('yolov8n.pt')\n",
    "        answer = sg.popup_yes_no(\"学習済みモデルを選択しますか？\")\n",
    "        if answer == 'No':\n",
    "            model = YOLO('yolov8n.pt')\n",
    "        elif answer == 'Yes':\n",
    "            model_path = sg.popup_get_file(\".ptファイルを選択してください\")\n",
    "            if model_path and model_path.endswith('.pt'):\n",
    "                sg.popup(f\"{model_path}が正常に読み込まれました\")\n",
    "                model = YOLO(model_path)\n",
    "            else:\n",
    "                sg.popup(\"読み込みに失敗しました。yolov8n.ptを使って学習を開始します\")\n",
    "                model = YOLO('yolov8n.pt')\n",
    "\n",
    "        # MPSデバイスの確認\n",
    "        device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "        sg.popup(f\"使用するデバイス: {device}\")\n",
    "\n",
    "        # トレーニングの実行\n",
    "        results = model.train(\n",
    "            data=f'{os.getcwd()}/new_yolo.yaml',  # データセットの設定\n",
    "            epochs=10,             # エポック数\n",
    "            batch=16,              # バッチサイズ\n",
    "            imgsz=416,             # 画像サイズ\n",
    "            workers=16,            # ワーカー数\n",
    "            device=device,         # デバイス設定（MPSまたはCPU使用）\n",
    "            cache=False\n",
    "        )\n",
    "\n",
    "        # 学習完了後のモデル保存\n",
    "        model_name = sg.popup_get_text(\"学習が完了しました。モデルの名前を記入してください\")\n",
    "        if model_name:\n",
    "            model.save(f'Dataset/model/{model_name}.pt')\n",
    "            sg.popup(f\"モデルが保存されました: {model_name}.pt\")\n",
    "        else:\n",
    "            sg.popup(\"モデル名が未入力のため、保存されませんでした。\")\n",
    "\n",
    "\n",
    "window.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
